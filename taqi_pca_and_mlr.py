# -*- coding: utf-8 -*-
"""TAQI_PCA_and_MLR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EXvr4jpT9RwowiggL2VDQQH428nHZazQ
"""

#import data
#Import dataset
import pandas as pd

#insert Air Quality CSV path here:
path = '______________________________'
data = pd.read_csv(path)

#Check data
data.head()
data.shape

data_aqi = data[['aqi', 'so2', 'co',
       'o3', 'o3_8hr', 'pm10', 'pm2.5', 'no2', 'nox', 'no', 'windspeed',
       'winddirec', 'co_8hr', 'pm2.5_avg', 'pm10_avg', 'so2_avg']]
data_aqi.head()

# Use pd.to_numeric with errors='coerce' to handle non-numeric values
data_aqi = data_aqi.apply(pd.to_numeric, errors='coerce')

# Check the data types after conversion
data_aqi.info()

# Show missing data
missing_data = data_aqi.isnull().sum()

# Display columns with missing data
missing_data = missing_data[missing_data > 0]
print('missing data')
print(missing_data)

# Remove rows where specified columns are null
data_aqi_cleaned = data_aqi.dropna()
data_aqi_cleaned.shape

#visullizing AQI
import matplotlib.pyplot as plt
import seaborn as sns

#AQI values for visualization
aqi_values = data_aqi_cleaned['aqi']

# Plot AQI distribution
plt.figure(figsize=(10, 6))
sns.histplot(aqi_values, bins=30, kde=True)
plt.title('Distribution of Air Quality Index (AQI)')
plt.xlabel('AQI')
plt.ylabel('Frequency')
plt.grid(True)
plt.tight_layout()
plt.show()

# Select features (independent variables) and target (dependent variable)
# Independent variables
X = data_aqi_cleaned[[ 'so2', 'co',
       'o3', 'o3_8hr', 'pm10', 'pm2.5', 'no2', 'nox', 'no', 'windspeed',
       'winddirec', 'co_8hr', 'pm2.5_avg', 'pm10_avg', 'so2_avg']]
# Dependent variable
y = data_aqi_cleaned['aqi']

# PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 1. Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

# 2. Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 3. Apply PCA to explain 95% of variance
pca = PCA(n_components=0.95)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Return shapes of PCA outputs and number of components retained
X_train_pca.shape, X_test_pca.shape, pca.n_components_

#Number of Principal Components Retained: 10 (enough to explain 95% of the variance)

pca.explained_variance_ratio_

#Scree Plot
#Plotting the Cumulative Summation of the Explained Variance
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(8, 5))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Scree Plot: Explained Variance vs. Components')
plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance Threshold')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()



"""MULTIPLE LINEAR REGRESSION"""

#Build regression model
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np
from sklearn.linear_model import LogisticRegression
# Initialize and train the regression model on PCA-transformed data
reg_model = LinearRegression()
reg_model.fit(X_train_pca, y_train)

# Predict on the test set
y_pred = reg_model.predict(X_test_pca)

# Evaluate model performance
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

# Output model performance metrics
r2, mae, mse, rmse

#RÂ² (Coefficient of Determination): 0.8771
#The model explains ~87.71% of the variance in AQI.

#MAE (Mean Absolute Error): 7.33
#On average, predictions are off by about 7 AQI units.

#MSE (Mean Squared Error): 106.679

#RMSE (Root Mean Squared Error): 10.328
#Typical prediction error is ~10.328 AQI units.

import matplotlib.pyplot as plt

# Create scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.6, edgecolor='k')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)

# Set plot labels and title
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values')
plt.grid(True)
plt.tight_layout()
plt.show()

#3D plot
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

# Ensure you have at least 2 PCA components
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Plot the 3D scatter: X_pca[0], X_pca[1], y_pred
ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], y_pred, c=y_pred, cmap='viridis', edgecolor='k', alpha=0.7)

# Axis labels
ax.set_xlabel('PCA Component 1')
ax.set_ylabel('PCA Component 2')
ax.set_zlabel('Predicted Target Value')
ax.set_title('3D Plot: PCA Components vs. Predictions')

plt.tight_layout()
plt.show()

#Apply transformed PCA dataset using elbow method to determine number cluster
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt


# Elbow method
inertia = []
k_range = range(1, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_train_pca)
    inertia.append(kmeans.inertia_)

# Plot the elbow curve
plt.figure(figsize=(8, 5))
plt.plot(k_range, inertia, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (Within-Cluster Sum of Squares)')
plt.xticks(k_range)
plt.grid(True)
plt.tight_layout()
plt.show()

"""From k = 4, the decrease in inertia becomes more gradual. The optimal number of clusters is 3"""